{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Seq_gKLxurYa",
   "metadata": {
    "id": "Seq_gKLxurYa"
   },
   "source": [
    "# MobileNetV3 Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c9e93c-3de4-4503-9b65-92dbd6b1d1f8",
   "metadata": {
    "id": "f9c9e93c-3de4-4503-9b65-92dbd6b1d1f8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from statistics import mode\n",
    "\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0_0NwS19QXQQ",
   "metadata": {
    "id": "0_0NwS19QXQQ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wZK2ablMrt7E",
   "metadata": {
    "id": "wZK2ablMrt7E"
   },
   "source": [
    "# File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tIn8afr3qz9E",
   "metadata": {
    "id": "tIn8afr3qz9E"
   },
   "outputs": [],
   "source": [
    "#Please change these here\n",
    "training_set_path = '/content/drive/My Drive/Project/train'\n",
    "validation_set_path = '/content/drive/My Drive/Project/val'\n",
    "testing_set_path = '/content/drive/My Drive/Project/val'\n",
    "\n",
    "#In Our second model, we are copying training images to another folder. Please set that training folder here\n",
    "destination_main_folder = '/content/drive/My Drive/Project/new_train'\n",
    "destination_side_folder = '/content/drive/My Drive/Project/new_val'\n",
    "destination_test_folder = '/content/drive/My Drive/Project/new_test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O764mDCvr7b7",
   "metadata": {
    "id": "O764mDCvr7b7"
   },
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db167d",
   "metadata": {
    "id": "21db167d"
   },
   "outputs": [],
   "source": [
    "# Setthing up MobilenetV3 Large Model\n",
    "model = models.mobilenet_v3_large(pretrained=True, progress=True)\n",
    "model.features[0][0] = nn.Conv2d(in_channels=1,out_channels=16,kernel_size=3,stride=2,padding=1,bias=False)\n",
    "model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, 7)  # 7 classes for skin cancer\n",
    "model.classifier[2] = nn.Dropout(p=0.4)\n",
    "\n",
    "\n",
    "# Accessing the block as direct accessing of the convolutional layer was not working\n",
    "old_dw = model.features[6].block[1][0]  # [6] = block index, [1] = depthwise conv block, [0] = actual Conv2d\n",
    "\n",
    "# Replace the layer with a dilated version\n",
    "model.features[6].block[1][0] = nn.Conv2d(\n",
    "    in_channels=old_dw.in_channels,\n",
    "    out_channels=old_dw.out_channels,\n",
    "    kernel_size=5,\n",
    "    stride=old_dw.stride,\n",
    "    padding=4,       # padding adjusted for dilation=2\n",
    "    dilation=2,\n",
    "    groups=old_dw.groups,\n",
    "    bias=old_dw.bias\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    for param in model.features[i].parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lDwQGdjVr_72",
   "metadata": {
    "id": "lDwQGdjVr_72"
   },
   "source": [
    "# Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b7f821-8f9f-45d7-a5a7-d0caa57f7a21",
   "metadata": {
    "id": "a0b7f821-8f9f-45d7-a5a7-d0caa57f7a21"
   },
   "outputs": [],
   "source": [
    "# Data augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.449], std=[0.226]),\n",
    "])\n",
    "\n",
    "# Data transform for validation/testing\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.449], std=[0.226])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OUPPzE7VsFGh",
   "metadata": {
    "id": "OUPPzE7VsFGh"
   },
   "source": [
    "# Forming the training and validation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c47d97f",
   "metadata": {
    "id": "3c47d97f"
   },
   "outputs": [],
   "source": [
    "# Paths to train and validation directories\n",
    "train_path = Path(training_set_path)\n",
    "valid_path = Path(validation_set_path)\n",
    "\n",
    "# Combine image files from both directories\n",
    "train_files = list(train_path.glob('*.jpg'))  # Adjust file extension if needed\n",
    "valid_files = list(valid_path.glob('*.jpg'))\n",
    "all_files = train_files + valid_files\n",
    "\n",
    "# Define indices for splitting\n",
    "train_idxs = list(range(len(train_files)))\n",
    "valid_idxs = list(range(len(train_files), len(all_files)))\n",
    "\n",
    "# Extract label from filename (e.g., \"3_xyz.jpg\" â†’ 2)\n",
    "def get_label(file):\n",
    "    return int(file.name.split('_')[0]) - 1\n",
    "\n",
    "# Define a custom PyTorch Dataset\n",
    "class SkinCancerDataset(Dataset):\n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and label\n",
    "        img_path = self.file_list[idx]\n",
    "        label = get_label(img_path)\n",
    "        img = Image.open(img_path) # Convert to grayscale\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SkinCancerDataset([all_files[i] for i in train_idxs], transform=train_transform)\n",
    "valid_dataset = SkinCancerDataset([all_files[i] for i in valid_idxs], transform=test_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ac67e-d972-4bb6-b119-dae589bc43e4",
   "metadata": {
    "id": "0d2ac67e-d972-4bb6-b119-dae589bc43e4"
   },
   "outputs": [],
   "source": [
    "# Extract class labels (targets) from the training dataset\n",
    "targets = [get_label(file) for file in train_files]  # Labels from training dataset files\n",
    "class_sample_count = np.array([targets.count(t) for t in np.unique(targets)])\n",
    "print(\"Class counts:\", class_sample_count)\n",
    "\n",
    "# Compute weights: inverse frequency of each class\n",
    "weights = 1. / class_sample_count\n",
    "samples_weight = np.array([weights[t] for t in targets])\n",
    "samples_weight = torch.from_numpy(samples_weight).float()\n",
    "\n",
    "# Create a WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(samples_weight, num_samples=len(samples_weight), replacement=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=4,pin_memory=True,persistent_workers=True)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4,pin_memory=True,persistent_workers=True)\n",
    "\n",
    "# Optional: Visualize a sample augmented training image\n",
    "sample_img, _ = train_dataset[0]\n",
    "plt.imshow(sample_img.squeeze(), cmap='gray')\n",
    "plt.title(\"Augmented Training Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w5zLAVAAsQSn",
   "metadata": {
    "id": "w5zLAVAAsQSn"
   },
   "source": [
    "# Defining Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd018d1",
   "metadata": {
    "id": "ecd018d1"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.CrossEntropyLoss()(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        focal_loss = self.alpha * (1 - pt)**self.gamma * BCE_loss\n",
    "        return focal_loss\n",
    "criterion = FocalLoss(alpha = 1, gamma = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16092ef3",
   "metadata": {
    "id": "16092ef3"
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import DataLoaders,Learner,accuracy\n",
    "# Create Fastai DataLoaders\n",
    "dls = DataLoaders(train_loader, val_loader)\n",
    "print(\"Data has been loaded\")\n",
    "# Assuming `dls` is your DataLoaders object, `model` is your model, and `criterion` your loss function\n",
    "learn = Learner(dls, model, loss_func=criterion, wd=1e-5, metrics=accuracy)\n",
    "print(\"Learner Created\")\n",
    "lr_min= learn.lr_find().valley\n",
    "print(f\"Suggested Learning Rates: valley={lr_min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443f8e4-551d-4a73-98e0-1d8f8f6a8c76",
   "metadata": {
    "id": "c443f8e4-551d-4a73-98e0-1d8f8f6a8c76"
   },
   "outputs": [],
   "source": [
    "loss_weights = torch.tensor(class_sample_count.sum()/(class_sample_count), dtype=torch.float32,device=device)\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr_min,weight_decay=1e-5)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c22f0f-fbb9-4bc9-9a68-99cdd2d58e10",
   "metadata": {
    "id": "55c22f0f-fbb9-4bc9-9a68-99cdd2d58e10"
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "val_loss_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    batch_iter = 0\n",
    "\n",
    "    # Training phase\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        batch_iter += 1\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_loss_history.append(epoch_loss)\n",
    "    train_acc_history.append(train_accuracy)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    running_val_loss = 0.0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs_val, labels_val in val_loader:\n",
    "            inputs_val = inputs_val.to(device)\n",
    "            labels_val = labels_val.to(device)\n",
    "\n",
    "            outputs_val = model(inputs_val)\n",
    "            val_loss = criterion(outputs_val, labels_val)  # Calculate validation loss\n",
    "            running_val_loss += val_loss.item()\n",
    "\n",
    "            _, preds_val = torch.max(outputs_val, 1)\n",
    "            correct_val += (preds_val == labels_val).sum().item()\n",
    "            total_val += labels_val.size(0)\n",
    "\n",
    "            true_labels.extend(labels_val.cpu().numpy())\n",
    "            pred_labels.extend(preds_val.cpu().numpy())\n",
    "\n",
    "    # Average validation loss for the epoch\n",
    "    epoch_val_loss = running_val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    val_loss_history.append(epoch_val_loss)\n",
    "    val_acc_history.append(val_accuracy)\n",
    "\n",
    "\n",
    "    # Update the scheduler based on validation loss\n",
    "    scheduler.step()\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}: Learning Rate = {current_lr}\")\n",
    "    # Print metrics for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f} | Train Acc: {train_accuracy:.2f}% | Val Loss: {epoch_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "    print(classification_report(true_labels, pred_labels, target_names=['0','1','2','3','4','5','6'], zero_division=0))\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b663b-2a45-4f50-82ed-1a6f2a2d8b32",
   "metadata": {
    "id": "c79b663b-2a45-4f50-82ed-1a6f2a2d8b32"
   },
   "outputs": [],
   "source": [
    "# Plot training loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs+1), train_loss_history, marker='*')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs+1), train_acc_history, marker='*', label='Train Acc')\n",
    "plt.plot(range(1, num_epochs+1), val_acc_history, marker='*', label='Val Acc')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Reds\", xticklabels=['0','1','2','3','4','5','6'], yticklabels=['0','1','2','3','4','5','6'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zfSnNGuuSAlt",
   "metadata": {
    "id": "zfSnNGuuSAlt"
   },
   "outputs": [],
   "source": [
    "# PLease change to testing_set_path, have set it to validation so that it runs\n",
    "test_path = Path(validation_set_path)\n",
    "\n",
    "# Combine image files from both directories\n",
    "test_files = list(test_path.glob('*.jpg'))\n",
    "\n",
    "# Define indices for splitting\n",
    "test_idxs = list(range(len(test_files)))\n",
    "\n",
    "# Create datasets\n",
    "test_dataset = SkinCancerDataset([all_files[i] for i in test_idxs], transform=test_transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "testing = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4,pin_memory=True,persistent_workers=True)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "class_correct = [0] * 7  # Initialize list to store correct predictions for each class\n",
    "class_total = [0] * 7   # Initialize list to store total samples for each class\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testing:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Calculate per-class accuracy\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i].item()\n",
    "            class_total[label] += 1\n",
    "            class_correct[label] += (predicted[i] == label).item()\n",
    "\n",
    "print(f'Overall Accuracy: {100 * correct / total}%')\n",
    "\n",
    "# Print per-class accuracy\n",
    "for i in range(7):\n",
    "    accuracy = 100 * class_correct[i] / class_total[i] if class_total[i] else 0  # Handle zero division\n",
    "    print(f'Accuracy of class {i}: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z-rsvbVCc0qe",
   "metadata": {
    "id": "z-rsvbVCc0qe"
   },
   "source": [
    "# **Model 2: MobileNetV2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QQn7FB23V-xP",
   "metadata": {
    "id": "QQn7FB23V-xP"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Path to trianing set\n",
    "source_folder = training_set_path\n",
    "\n",
    "#Destination For New folder\n",
    "\n",
    "# Make sure the destination main folder exists\n",
    "os.makedirs(destination_main_folder, exist_ok=True)\n",
    "\n",
    "# Loop through each file in the source folder\n",
    "for filename in os.listdir(source_folder):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):  # Add more extensions if needed\n",
    "        try:\n",
    "            # Extract class number (before underscore)\n",
    "            class_num = filename.split('_')[0]\n",
    "            class_folder_name = f'class{class_num}'\n",
    "\n",
    "            # Create the class subfolder inside the new main folder\n",
    "            class_folder_path = os.path.join(destination_main_folder, class_folder_name)\n",
    "            os.makedirs(class_folder_path, exist_ok=True)\n",
    "\n",
    "            # Copy the file\n",
    "            src_path = os.path.join(source_folder, filename)\n",
    "            dst_path = os.path.join(class_folder_path, filename)\n",
    "            shutil.copy2(src_path, dst_path)  # copy2 preserves metadata\n",
    "\n",
    "            print(f'Copied {filename} to {class_folder_name}')\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {filename}: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T0zKER5CxUvx",
   "metadata": {
    "id": "T0zKER5CxUvx"
   },
   "outputs": [],
   "source": [
    "# Path to trianing set\n",
    "source_folder = validation_set_path\n",
    "\n",
    "#Destination For New folder\n",
    "\n",
    "# Make sure the destination main folder exists\n",
    "os.makedirs(destination_side_folder, exist_ok=True)\n",
    "\n",
    "# Loop through each file in the source folder\n",
    "for filename in os.listdir(source_folder):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):  # Add more extensions if needed\n",
    "        try:\n",
    "            # Extract class number (before underscore)\n",
    "            class_num = filename.split('_')[0]\n",
    "            class_folder_name = f'class{class_num}'\n",
    "\n",
    "            # Create the class subfolder inside the new main folder\n",
    "            class_folder_path = os.path.join(destination_side_folder, class_folder_name)\n",
    "            os.makedirs(class_folder_path, exist_ok=True)\n",
    "\n",
    "            # Copy the file\n",
    "            src_path = os.path.join(source_folder, filename)\n",
    "            dst_path = os.path.join(class_folder_path, filename)\n",
    "            shutil.copy2(src_path, dst_path)  # copy2 preserves metadata\n",
    "\n",
    "            print(f'Copied {filename} to {class_folder_name}')\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {filename}: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oXkzG4UWWlr2",
   "metadata": {
    "id": "oXkzG4UWWlr2"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, Rotate, RandomBrightnessContrast,\n",
    "    ShiftScaleRotate, GaussianBlur, RandomShadow, RGBShift\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "\n",
    "source_dir = destination_main_folder\n",
    "target_count = 2000                # Target samples per class\n",
    "placeholder = \"_synthetic\"        # To tag synthetic images so we can delete these later\n",
    "\n",
    "# Data Augmentations for synthetic images\n",
    "augment = Compose([\n",
    "    HorizontalFlip(p=0.5),\n",
    "    Rotate(limit=30, p=0.5),\n",
    "    RandomBrightnessContrast(p=0.5),\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=20, p=0.5),\n",
    "    GaussianBlur(p=0.3),\n",
    "    RGBShift(p=0.3),\n",
    "    RandomShadow(p=0.3)\n",
    "])\n",
    "\n",
    "#creating synthetic images\n",
    "for class_dir in os.listdir(source_dir):\n",
    "    class_path = os.path.join(source_dir, class_dir)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    images = glob(os.path.join(class_path, \"*.jpg\")) + \\\n",
    "             glob(os.path.join(class_path, \"*.png\")) + \\\n",
    "             glob(os.path.join(class_path, \"*.jpeg\"))\n",
    "\n",
    "    current_count = len(images)\n",
    "    print(f\"Class {class_dir}: {current_count} images\")\n",
    "\n",
    "    if current_count >= target_count:\n",
    "        continue\n",
    "    needed = target_count - current_count\n",
    "    print(f\"â†’ Generating {needed} synthetic images for class {class_dir}\")\n",
    "\n",
    "    for i in tqdm(range(needed)):\n",
    "        img_path = random.choice(images)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        augmented = augment(image=image)['image']\n",
    "        augmented = cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        base_name = os.path.basename(img_path)\n",
    "        name, ext = os.path.splitext(base_name)\n",
    "        new_name = f\"{name}_{placeholder}_{i}{ext}\"\n",
    "        new_path = os.path.join(class_path, new_name)\n",
    "\n",
    "        cv2.imwrite(new_path, augmented)\n",
    "\n",
    "print(\"\\nSynthetic image generation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lajpcMpfWtzn",
   "metadata": {
    "id": "lajpcMpfWtzn"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 5)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalization for ImageNet pre-trained models\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1s9h1Be9cHuN",
   "metadata": {
    "id": "1s9h1Be9cHuN"
   },
   "outputs": [],
   "source": [
    "# Paths to training and validation datasets\n",
    "train_dataset = ImageFolder(root = destination_main_folder, transform = train_transforms)\n",
    "val_dataset = ImageFolder(root = destination_side_folder, transform = val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VYpk9_HXcIVx",
   "metadata": {
    "id": "VYpk9_HXcIVx"
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UdM8V5gYcKRN",
   "metadata": {
    "id": "UdM8V5gYcKRN"
   },
   "outputs": [],
   "source": [
    "# Calculate class counts for weighted sampling\n",
    "targets = [s[1] for s in train_dataset.samples]\n",
    "class_sample_count = np.array([targets.count(t) for t in np.unique(targets)])\n",
    "print(\"Class counts:\", class_sample_count)\n",
    "\n",
    "# Compute weights: inverse frequency\n",
    "weights = 1. / class_sample_count\n",
    "samples_weight = np.array([weights[t] for t in targets])\n",
    "samples_weight = torch.from_numpy(samples_weight).float()\n",
    "sampler = WeightedRandomSampler(samples_weight, num_samples=len(samples_weight), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JKw5U2IFcOCX",
   "metadata": {
    "id": "JKw5U2IFcOCX"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler, num_workers=4, persistent_workers=True,  pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle = False, num_workers=4, persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3KkqAh7McQ-F",
   "metadata": {
    "id": "3KkqAh7McQ-F"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2 model and modify it\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "model.classifier[1] = nn.Linear(model.last_channel, len(train_dataset.classes))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6iUgAw-ScRtx",
   "metadata": {
    "id": "6iUgAw-ScRtx"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rp0MclMLcUq2",
   "metadata": {
    "id": "rp0MclMLcUq2"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model_with_accuracy(model, criterion, optimizer, scheduler, num_epochs=20):\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_train += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = correct_train.double() / len(train_dataset)\n",
    "        train_loss_history.append(epoch_loss)\n",
    "        train_acc_history.append(epoch_acc.item())\n",
    "\n",
    "        print(f'Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_val += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_epoch_loss = val_loss / len(val_dataset)\n",
    "        val_epoch_acc = correct_val.double() / len(val_dataset)\n",
    "        val_loss_history.append(val_epoch_loss)  # Append validation loss to the list\n",
    "        val_acc_history.append(val_epoch_acc.item())\n",
    "\n",
    "        print(f'Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_acc:.4f}')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    return train_loss_history, val_loss_history, train_acc_history, val_acc_history  # Return loss histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dvtuCjq4cZ36",
   "metadata": {
    "id": "dvtuCjq4cZ36"
   },
   "outputs": [],
   "source": [
    "train_loss_history, val_loss_history, train_acc_history, val_acc_history = train_model_with_accuracy(model, criterion, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pcEtrqc_ccAG",
   "metadata": {
    "id": "pcEtrqc_ccAG"
   },
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_acc_history, label='Train Accuracy')\n",
    "plt.plot(val_acc_history, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jEmw5tTvcdqX",
   "metadata": {
    "id": "jEmw5tTvcdqX"
   },
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B_28dLQJchg-",
   "metadata": {
    "id": "B_28dLQJchg-"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, data_loader, dataset_type ):\n",
    "  model.eval()\n",
    "  all_preds = []\n",
    "  all_labels = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for inputs, labels in data_loader:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          outputs = model(inputs)\n",
    "          _, preds = torch.max(outputs, 1)\n",
    "          all_preds.extend(preds.cpu().numpy())\n",
    "          all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "  # Calculate confusion matrix\n",
    "  cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "  # Plot the confusion matrix using seaborn\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\") #added cmap value\n",
    "  plt.title(f\"Confusion Matrix for {dataset_type} Set\")\n",
    "\n",
    "\n",
    "  # Calculate per-class accuracy\n",
    "  class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "  # Print per-class accuracy\n",
    "  print(f\"--- Class Accuracy for {dataset_type} Set ---\")  # Added distinction\n",
    "  for i, accuracy in enumerate(class_accuracy):\n",
    "      print(f\"Accuracy for class {train_dataset.classes[i]}: {accuracy*100:.4f}\")\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()  # Display the plot immediately to separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0NTvO2GucjiJ",
   "metadata": {
    "id": "0NTvO2GucjiJ"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, train_loader, dataset_type=\"Training\")  # Confusion matrix for training set\n",
    "plot_confusion_matrix(model, val_loader, dataset_type=\"Validation\")  # Confusion matrix for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MmQEqTxldQct",
   "metadata": {
    "id": "MmQEqTxldQct"
   },
   "outputs": [],
   "source": [
    "print(f\"Final Validation Accuracy: {val_acc_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ltL-Chn3lpGX",
   "metadata": {
    "id": "ltL-Chn3lpGX"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree(destination_main_folder)\n",
    "    print(f'Deleted folder: {destination_main_folder}')\n",
    "except Exception as e:\n",
    "    print(f'Error deleting folder {destination_main_folder}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ufq80HW6xm94",
   "metadata": {
    "id": "ufq80HW6xm94"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree(destination_side_folder)\n",
    "    print(f'Deleted folder: {destination_side_folder}')\n",
    "except Exception as e:\n",
    "    print(f'Error deleting folder {destination_side_folder}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rfxnr2qalfra",
   "metadata": {
    "id": "rfxnr2qalfra"
   },
   "source": [
    "## **Test Block:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Aea-OaXO13R1",
   "metadata": {
    "id": "Aea-OaXO13R1"
   },
   "outputs": [],
   "source": [
    "# Path to trianing set\n",
    "source_folder = testing_set_path\n",
    "\n",
    "#Destination For New folder\n",
    "\n",
    "# Make sure the destination main folder exists\n",
    "os.makedirs(destination_test_folder, exist_ok=True)\n",
    "\n",
    "# Loop through each file in the source folder\n",
    "for filename in os.listdir(source_folder):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):  # Add more extensions if needed\n",
    "        try:\n",
    "            # Extract class number (before underscore)\n",
    "            class_num = filename.split('_')[0]\n",
    "            class_folder_name = f'class{class_num}'\n",
    "\n",
    "            # Create the class subfolder inside the new main folder\n",
    "            class_folder_path = os.path.join(destination_test_folder, class_folder_name)\n",
    "            os.makedirs(class_folder_path, exist_ok=True)\n",
    "\n",
    "            # Copy the file\n",
    "            src_path = os.path.join(source_folder, filename)\n",
    "            dst_path = os.path.join(class_folder_path, filename)\n",
    "            shutil.copy2(src_path, dst_path)  # copy2 preserves metadata\n",
    "\n",
    "            print(f'Copied {filename} to {class_folder_name}')\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {filename}: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wtb4X9rVgBAA",
   "metadata": {
    "id": "Wtb4X9rVgBAA"
   },
   "outputs": [],
   "source": [
    "new_test_dataset = ImageFolder(root = destination_test_folder, transform = val_transforms)\n",
    "new_test_loader = DataLoader(new_test_dataset, batch_size=32, shuffle = False, num_workers=4, persistent_workers=True, pin_memory=True)\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "class_correct = [0] * 7  # Initialize list to store correct predictions for each class\n",
    "class_total = [0] * 7   # Initialize list to store total samples for each class\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in new_test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Calculate per-class accuracy\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i].item()\n",
    "            class_total[label] += 1\n",
    "            class_correct[label] += (predicted[i] == label).item()\n",
    "\n",
    "print(f'Overall Accuracy: {100 * correct / total}%')\n",
    "\n",
    "# Print per-class accuracy\n",
    "for i in range(7):\n",
    "    accuracy = 100 * class_correct[i] / class_total[i] if class_total[i] else 0  # Handle zero division\n",
    "    print(f'Accuracy of class {i}: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
